# LLM Preprocessing Examples

This document provides examples of using the LLM preprocessing features with the Edge TTS API.

## Overview

The API now supports two optional LLM preprocessing modes:

1. **Text Optimization for TTS** (`optimize_for_tts`): Cleans and optimizes input text for better TTS output
2. **SSML Markup Generation** (`add_ssml_markup`): Automatically adds SSML tags for natural pronunciation

Both features require an OpenAI-compatible LLM endpoint and API key.

## Configuration

You need to provide:
- `llm_api_key`: Your API key for the LLM service
- `llm_endpoint`: The OpenAI-compatible endpoint URL (e.g., `https://api.openai.com/v1/chat/completions`)

## Example 1: Text Optimization

Optimize text by replacing uncommon characters, simplifying lists, and expanding abbreviations.

```bash
curl -X POST https://your-worker.workers.dev/v1/audio/speech_subtitles \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello! This is a test w/ some abbrev. & special chars #1",
    "voice": "en-US-EmmaMultilingualNeural",
    "llm_api_key": "sk-your-api-key",
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "optimize_for_tts": true
  }'
```

**Before:** `"Hello! This is a test w/ some abbrev. & special chars #1"`

**After (example):** `"Hello! This is a test with some abbreviations and special characters number one"`

## Example 2: SSML Markup Generation

Automatically add SSML tags for natural pauses, emphasis, and proper pronunciation.

```bash
curl -X POST https://your-worker.workers.dev/v1/audio/speech_subtitles \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Hello, world! This is very important. The meeting is on January 15th at 3pm.",
    "voice": "en-US-EmmaMultilingualNeural",
    "llm_api_key": "sk-your-api-key",
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "add_ssml_markup": true
  }'
```

**Before:** `"Hello, world! This is very important. The meeting is on January 15th at 3pm."`

**After (example):**
```xml
<speak>
  Hello, world!
  <break strength="medium"/>
  This is <emphasis level="strong">very important</emphasis>.
  <break strength="medium"/>
  The meeting is on <say-as interpret-as="date" format="md">January 15th</say-as> 
  at <say-as interpret-as="time" format="hm">3pm</say-as>.
</speak>
```

## Example 3: Combined Processing

Use both optimization and SSML markup together for the best results.

```bash
curl -X POST https://your-worker.workers.dev/v1/audio/speech_subtitles \
  -H "Content-Type: application/json" \
  -d '{
    "input": "TODO: Call John @ 555-1234 re: Q1 results (ASAP!)",
    "voice": "en-US-EmmaMultilingualNeural",
    "llm_api_key": "sk-your-api-key",
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "optimize_for_tts": true,
    "add_ssml_markup": true
  }'
```

**Processing flow:**
1. First, text is optimized: `"TODO: Call John @ 555-1234 re: Q1 results (ASAP!)"` â†’ `"To do: Call John at five five five one two three four regarding quarter one results as soon as possible"`
2. Then, SSML markup is added with appropriate emphasis and breaks

## Example 4: Using with Raw Audio Endpoint

The LLM preprocessing also works with the `/v1/audio/speech` endpoint:

```bash
curl -X POST https://your-worker.workers.dev/v1/audio/speech \
  -H "Content-Type: application/json" \
  -d '{
    "input": "Check the README.md file ASAP!",
    "voice": "en-US-EmmaMultilingualNeural",
    "llm_api_key": "sk-your-api-key",
    "llm_endpoint": "https://api.openai.com/v1/chat/completions",
    "optimize_for_tts": true,
    "add_ssml_markup": true
  }' --output speech.mp3
```

## Using the Demo UI

The demo interface at `/` includes a toggleable LLM preprocessing section:

1. Check "ðŸ¤– Enable LLM Preprocessing"
2. Enter your LLM endpoint URL
3. Enter your API key
4. Select the desired preprocessing options:
   - âœ… Optimize text for TTS
   - âœ… Add SSML markup
5. Click "Generate Speech & Subtitles"

Your settings are automatically saved to LocalStorage for convenience.

## Error Handling

If LLM preprocessing fails, the API will return a 500 error with details:

```json
{
  "error": "LLM text optimization failed",
  "message": "LLM API request failed: 401 Unauthorized"
}
```

Common errors:
- Invalid API key
- Endpoint not reachable
- Invalid SSML generated by LLM
- LLM API rate limits exceeded

## System Prompts

### Text Optimization Prompt

The LLM uses a specialized system prompt to:
- Replace uncommon characters with spoken equivalents
- Simplify lists and bullet points to natural prose
- Expand abbreviations and acronyms
- Fix typos and formatting issues
- Ensure punctuation supports natural speech rhythm

### SSML Markup Prompt

The LLM uses a specialized system prompt to:
- Add `<break>` tags for natural pauses
- Add `<emphasis>` tags for important words
- Add `<prosody>` tags for rate/pitch/volume adjustments
- Add `<say-as>` tags for dates, times, numbers
- Validate SSML structure and tag nesting

## Best Practices

1. **Use text optimization** when dealing with:
   - Technical documentation with abbreviations
   - Lists and bullet points
   - Text with special characters or symbols
   - Mixed formatting (markdown, etc.)

2. **Use SSML markup** when you need:
   - Natural pauses and pacing
   - Emphasis on specific words
   - Proper pronunciation of dates, times, numbers
   - Enhanced expressiveness

3. **Use both together** for the best results with complex text

4. **Keep API keys secure**: Store them in environment variables or LocalStorage, never in code

5. **Handle errors gracefully**: Always check for LLM preprocessing errors and provide fallbacks

## Alternative LLM Providers

The API works with any OpenAI-compatible endpoint:

- **OpenAI**: `https://api.openai.com/v1/chat/completions`
- **Azure OpenAI**: `https://YOUR-RESOURCE.openai.azure.com/openai/deployments/YOUR-DEPLOYMENT/chat/completions?api-version=2024-02-15-preview`
- **Local models** (via LM Studio, Ollama, etc.): `http://localhost:1234/v1/chat/completions`
- **Other providers**: Any service that implements the OpenAI chat completions API

## Performance Considerations

- LLM preprocessing adds latency (typically 1-3 seconds per request)
- Consider caching optimized/marked-up text if processing the same content repeatedly
- Use smaller/faster models if latency is critical
- The optimization runs sequentially (optimize â†’ add SSML) when both are enabled
